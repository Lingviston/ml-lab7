{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_file_path = \"data/iris.data\"\n",
    "\n",
    "iris_classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_class_data(file_path, iris_class):\n",
    "    matrix = list()\n",
    "    with open(file_path, newline='') as file:\n",
    "        reader = csv.reader(file, delimiter=',', quotechar='|')\n",
    "        for row in reader:\n",
    "            if row[4] == iris_class:\n",
    "                matrix_row = [1] + [float(x) for x in row[:-1]]\n",
    "                matrix.append(matrix_row)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_sublist_indeces(full_list, sublist_size):\n",
    "    \n",
    "    full_list_size = len(full_list)\n",
    "    assert full_list_size >= sublist_size\n",
    "    \n",
    "    indeces = range(0, full_list_size)\n",
    "    sub_list_indeces = list()\n",
    "    while(len(sub_list_indeces) != sublist_size):\n",
    "        random_element_index = random.choice(indeces)\n",
    "        if random_element_index not in sub_list_indeces:\n",
    "            sub_list_indeces.append(random_element_index)\n",
    "    \n",
    "    return sub_list_indeces\n",
    "\n",
    "def split_to_learn_and_test(sample, test_sample_size):\n",
    "    \n",
    "    test_sample_indeces = get_random_sublist_indeces(sample, test_sample_size)\n",
    "    \n",
    "    learn_sample = list()\n",
    "    test_sample = list()\n",
    "    for i in range(0, len(sample)):\n",
    "        if i in test_sample_indeces:\n",
    "            test_sample.append(sample[i])\n",
    "        else:\n",
    "            learn_sample.append(sample[i])\n",
    "    return (learn_sample, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the whole dataset\n",
    "iris_setosa = read_class_data(iris_file_path, iris_classes[0])\n",
    "iris_versicolor = read_class_data(iris_file_path, iris_classes[1])\n",
    "iris_virginica = read_class_data(iris_file_path, iris_classes[2])\n",
    "\n",
    "# Splitting the dataset into samples for learning and for testing\n",
    "(iris_setosa_learn, iris_setosa_test) = split_to_learn_and_test(iris_setosa, len(iris_setosa) // 10)\n",
    "(iris_versicolor_learn, iris_versicolor_test) = split_to_learn_and_test(iris_versicolor, len(iris_versicolor) // 10)\n",
    "(iris_virginica_learn, iris_virginica_test) = split_to_learn_and_test(iris_virginica, len(iris_virginica) // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_scalar_product(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    product = 0\n",
    "    \n",
    "    for i in range(0, len(x)):\n",
    "        product += x[i] * y[i]\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(x, w):\n",
    "    value = calculate_scalar_product(x, w)\n",
    "    point = 1 / (1 + math.exp(-value))\n",
    "    \n",
    "    if (point < 0.5):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "def calculate_error(w, X, Y):\n",
    "    assert len(X) == len(Y)\n",
    "    assert len(X) > 0\n",
    "    \n",
    "    count = len(X)\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        point = classify(X[i], w)\n",
    "        if point != Y[i]:\n",
    "            incorrect_count += 1\n",
    "                \n",
    "    return incorrect_count / count\n",
    "\n",
    "\n",
    "def calculate_loss(w, X, Y, tau):\n",
    "    assert len(X) == len(Y)\n",
    "    assert len(X) > 0\n",
    "    \n",
    "    count = len(X)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(0, count):\n",
    "        value = Y[i] * calculate_scalar_product(X[i], w)\n",
    "        loss += math.log(1 + math.exp(-value))\n",
    "    \n",
    "    loss = loss / count + tau * calculate_scalar_product(w, w)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_for_points(w, X, Y, tau):\n",
    "    assert len(X) == len(Y)\n",
    "    assert len(w) == len(X[0])\n",
    "    \n",
    "    grad =  [0 for x in range(0, len(w))]\n",
    "    \n",
    "    for i in range(0, len(X)):\n",
    "        for j in range(0, len(grad)):\n",
    "            exp_power = Y[i] * calculate_scalar_product(w, X[i])\n",
    "            grad[j] += -Y[i]*X[i][j] / (1 + math.exp(exp_power))\n",
    "            \n",
    "    # normalize and regularize\n",
    "    for i in range(0, len(grad)):\n",
    "        grad[i] = grad[i] / len(X) + tau * 2 * w[i]\n",
    "    return grad\n",
    "\n",
    "def bgd_learn(tau, etta, T, X, Y):\n",
    "    w = [0 for x in range(0, len(X[0]))]\n",
    "    \n",
    "    for t in range(0, T):\n",
    "        grad = calculate_gradient_for_points(w, X, Y, tau)\n",
    "        \n",
    "        for i in range(0, len(w)):\n",
    "            w[i] -= etta * grad[i]\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_for_point(w, x, y, tau):\n",
    "    assert len(w) == len(x)\n",
    "    \n",
    "    grad =  [0 for x in range(0, len(w))]\n",
    "    \n",
    "    exp_power = y * calculate_scalar_product(w, x)\n",
    "    for i in range(0, len(grad)):\n",
    "        grad[i] += -y*x[i] / (1 + math.exp(exp_power)) + tau * 2 * w[i]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def sgd_learn(tau, etta, T, X, Y):\n",
    "    w = [0 for x in range(0, len(X[0]))]\n",
    "    \n",
    "    random_indeces = range(0, len(X))\n",
    "    for t in range(0, T):\n",
    "        random_index = random.choice(random_indeces)\n",
    "        \n",
    "        x = X[random_index]\n",
    "        y = Y[random_index]\n",
    "        \n",
    "        grad = calculate_gradient_for_point(w, x, y, tau)\n",
    "        \n",
    "        for i in range(0, len(w)):\n",
    "            w[i] -= etta * grad[i]\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_learn(folds_count, target_class_sample, other_classes_sample, algorithm):\n",
    "    target_validation_sample_size = len(target_class_sample) // folds_count\n",
    "    other_validation_sample_size = len(other_classes_sample) // folds_count\n",
    "    \n",
    "    # if it possible\n",
    "    target_class_sample = [x for x in target_class_sample]\n",
    "    target_processed_sample = list()\n",
    "    target_validation_sample = list()\n",
    "    \n",
    "    other_classes_sample = [x for x in other_classes_sample]\n",
    "    other_processed_sample = list()\n",
    "    other_validation_sample = list()\n",
    "    \n",
    "    average_error = 0\n",
    "    for fold in range(0, folds_count):\n",
    "        (target_class_sample, target_validation_sample) = split_to_learn_and_test(target_class_sample, target_validation_sample_size)\n",
    "        (other_classes_sample, other_validation_sample) = split_to_learn_and_test(other_classes_sample, other_validation_sample_size)\n",
    "        \n",
    "        target_learn_sample = target_class_sample + target_processed_sample\n",
    "        other_learn_sample = other_classes_sample + other_processed_sample\n",
    "        \n",
    "        learn_sample = target_learn_sample + other_learn_sample\n",
    "        learn_markers = [1 if x < len(target_learn_sample) else -1 for x in range(0, len(learn_sample))]\n",
    "        \n",
    "        validation_sample = target_validation_sample + other_validation_sample\n",
    "        validation_markers = [1 if x < len(target_validation_sample) else -1 for x in range(0, len(validation_sample))]\n",
    "        \n",
    "        # TODO: learn with sgd or bgd\n",
    "        w = algorithm(learn_sample, learn_markers)\n",
    "        \n",
    "        average_error += calculate_error(w, validation_sample, validation_markers)\n",
    "        \n",
    "        target_processed_sample += target_validation_sample\n",
    "        other_processed_sample += other_validation_sample\n",
    "    \n",
    "    average_error = average_error / folds_count\n",
    "    \n",
    "    return average_error\n",
    "\n",
    "\n",
    "def k_fold_learn_new(folds_count, target_class_sample, other_classes_sample, tau_values, etta_values, T_values, algorithm):\n",
    "    assert len(target_class_sample) % folds_count == 0\n",
    "    assert len(other_classes_sample) % folds_count == 0\n",
    "    \n",
    "    target_validation_sample_size = len(target_class_sample) // folds_count\n",
    "    other_validation_sample_size = len(other_classes_sample) // folds_count\n",
    "    \n",
    "    # preparing data for splitting to k parts\n",
    "    target_class_sample = [x for x in target_class_sample]\n",
    "    target_processed_sample = list()\n",
    "    target_validation_sample = list()\n",
    "    \n",
    "    other_classes_sample = [x for x in other_classes_sample]\n",
    "    other_processed_sample = list()\n",
    "    other_validation_sample = list()\n",
    "    \n",
    "    fold_learn_samples = list()\n",
    "    fold_learn_markers = list()\n",
    "    fold_validation_samples = list()\n",
    "    fold_validation_markers = list()\n",
    "    \n",
    "    # splitting samples to k folds\n",
    "    for fold in range(0, folds_count):\n",
    "        (target_class_sample, target_validation_sample) = split_to_learn_and_test(target_class_sample, target_validation_sample_size)\n",
    "        (other_classes_sample, other_validation_sample) = split_to_learn_and_test(other_classes_sample, other_validation_sample_size)\n",
    "        \n",
    "        target_learn_sample = target_class_sample + target_processed_sample\n",
    "        other_learn_sample = other_classes_sample + other_processed_sample\n",
    "        \n",
    "        learn_sample = target_learn_sample + other_learn_sample\n",
    "        learn_markers = [1 if x < len(target_learn_sample) else -1 for x in range(0, len(learn_sample))]\n",
    "        \n",
    "        validation_sample = target_validation_sample + other_validation_sample\n",
    "        validation_markers = [1 if x < len(target_validation_sample) else -1 for x in range(0, len(validation_sample))]\n",
    "        \n",
    "        fold_learn_samples.append(learn_sample)\n",
    "        fold_learn_markers.append(learn_markers)\n",
    "        fold_validation_samples.append(validation_sample)\n",
    "        fold_validation_markers.append(validation_markers)\n",
    "        \n",
    "        target_processed_sample += target_validation_sample\n",
    "        other_processed_sample += other_validation_sample\n",
    "    \n",
    "    folds_data = list(zip(fold_learn_samples, fold_learn_markers, fold_validation_samples, fold_validation_markers))\n",
    "\n",
    "    assert len(folds_data) == folds_count\n",
    "    \n",
    "    # we have k folds, now start iterating through params\n",
    "    k_fold_results = list()\n",
    "    for tau in tau_values:\n",
    "        for etta in etta_values:\n",
    "            for T in T_values:\n",
    "                average_loss = 0\n",
    "                \n",
    "                for learn_sample, learn_markers, validation_sample, validation_markers in folds_data:\n",
    "                    w = algorithm(tau, etta, T, learn_sample, learn_markers)\n",
    "                    loss = calculate_loss(w, validation_sample, validation_markers, tau)\n",
    "                    \n",
    "                    average_loss += loss\n",
    "                \n",
    "                average_loss = average_loss / folds_count\n",
    "                k_fold_results.append((tau, etta, T, average_loss))\n",
    "                # print(\"Calcualted weights for tau = \", tau,  \" etta = \", etta, \" T = \", T, \" with average true risk \", average_error)\n",
    "                \n",
    "    # choosing the best params combination\n",
    "    \n",
    "    min_error = min(k_fold_results, key=itemgetter(3))\n",
    "    \n",
    "    return min_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for  BGD  =  (0.1, 0.05, 300)\n",
      "Best weights =  [0.10145171572954542, 0.15077808242564367, 0.578433643974476, -0.891183227950756, -0.38260881831657784]\n",
      "Classification error for  Iris-Setonza vs Versicolor  =  0.0\n",
      "Best params for  SGD  =  (0.1, 0.02, 200)\n",
      "Best weights =  [0.07827409498993394, 0.1113815152272994, 0.5342955931868008, -0.7986355188409722, -0.3435533564816997]\n",
      "Classification error for  Iris-Setonza vs Versicolor  =  0.0\n",
      "Best params for  BGD  =  (0.1, 0.05, 300)\n",
      "Best weights =  [0.1259670168873629, 0.2300184162825966, 0.5183008343079073, -0.8200130809365441, -0.4168158545529107]\n",
      "Classification error for  Iris-Setonza vs Virginica  =  0.0\n",
      "Best params for  SGD  =  (0.1, 0.05, 200)\n",
      "Best weights =  [0.1355288644935789, 0.2663778741450885, 0.548108692372022, -0.8402827962702926, -0.41861143490382735]\n",
      "Classification error for  Iris-Setonza vs Virginica  =  0.0\n",
      "Best params for  BGD  =  (0.1, 0.1, 200)\n",
      "Best weights =  [0.1619290110667123, 0.33823807190510186, 0.23487903198243001, -0.48137633815015507, -0.43168950099379105]\n",
      "Classification error for  Iris-Versicolor vs Virginica  =  0.0\n",
      "Best params for  SGD  =  (0.1, 0.01, 300)\n",
      "Best weights =  [0.07717404903135441, 0.16897216037677829, 0.11887261470971906, -0.2834324492741104, -0.24575389024788685]\n",
      "Classification error for  Iris-Versicolor vs Virginica  =  0.4\n",
      "Best params for  BGD  =  (0.1, 0.05, 300)\n",
      "Best weights =  [-0.10145171572954545, -0.15077808242564375, -0.578433643974476, 0.8911832279507561, 0.3826088183165779]\n",
      "Classification error for  Iris-Versicolor vs Setonza  =  0.0\n",
      "Best params for  SGD  =  (0.1, 0.02, 300)\n",
      "Best weights =  [-0.10533968690099085, -0.14315985022556957, -0.5673929862837632, 0.8756281462690578, 0.37484948403970214]\n",
      "Classification error for  Iris-Versicolor vs Setonza  =  0.0\n",
      "Best params for  BGD  =  (0.1, 0.1, 200)\n",
      "Best weights =  [-0.1619290110667124, -0.338238071905102, -0.23487903198242985, 0.48137633815015496, 0.431689500993791]\n",
      "Classification error for  Iris-Virginica vs Iris-Versicolor  =  0.0\n",
      "Best params for  SGD  =  (0.1, 0.02, 200)\n",
      "Best weights =  [-0.09904535420934688, -0.19713412839941832, -0.13967960819393999, 0.32215536385087806, 0.2839512095877895]\n",
      "Classification error for  Iris-Virginica vs Iris-Versicolor  =  0.3\n",
      "Best params for  BGD  =  (0.1, 0.2, 50)\n",
      "Best weights =  [-0.12550163914237983, -0.22916492800350402, -0.5183398830967504, 0.8189939136455189, 0.41538972755066333]\n",
      "Classification error for  Iris-Virginica vs Iris-Setonza  =  0.0\n",
      "Best params for  SGD  =  (0.1, 0.02, 300)\n",
      "Best weights =  [-0.12914827796866973, -0.24317848188889585, -0.5422258121793777, 0.7950874893112367, 0.3954004769899509]\n",
      "Classification error for  Iris-Virginica vs Iris-Setonza  =  0.0\n"
     ]
    }
   ],
   "source": [
    "def calcualte_model(target_class_name, alg_name, target_learn, other_learn, target_test, other_test, algorithm):\n",
    "    tau_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    etta_value = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5]\n",
    "    T_values = [20,50,100,200,300]\n",
    "    (tau, etta, T, average_error) = k_fold_learn_new(9, target_learn, other_learn, tau_values, etta_value, T_values, algorithm)\n",
    "    learn_set = target_learn + other_learn\n",
    "    test_set = target_test + other_test\n",
    "    Y_learn = [1 if x < len(target_learn) else -1 for x in range(0, len(learn_set))]\n",
    "    Y_test = [1 if x < len(target_test) else -1 for x in range(0, len(test_set))]\n",
    "    w = algorithm(tau, etta, T, learn_set, Y_learn)\n",
    "    print(\"Best params for \", alg_name, \" = \", (tau, etta, T))\n",
    "    print(\"Best weights = \", w)\n",
    "    \n",
    "    test_error = calculate_error(w, test_set, Y_test)\n",
    "    print(\"Classification error for \", target_class_name, \" = \", test_error)\n",
    "\n",
    "calcualte_model(\"Iris-Setonza vs Versicolor\", \"BGD\", iris_setosa_learn, iris_versicolor_learn, iris_setosa_test, iris_versicolor_test, bgd_learn)\n",
    "calcualte_model(\"Iris-Setonza vs Versicolor\", \"SGD\", iris_setosa_learn, iris_versicolor_learn, iris_setosa_test, iris_versicolor_test, sgd_learn)\n",
    "calcualte_model(\"Iris-Setonza vs Virginica\", \"BGD\", iris_setosa_learn, iris_virginica_learn, iris_setosa_test, iris_virginica_test, bgd_learn)\n",
    "calcualte_model(\"Iris-Setonza vs Virginica\", \"SGD\", iris_setosa_learn, iris_virginica_learn, iris_setosa_test, iris_virginica_test, sgd_learn)\n",
    "calcualte_model(\"Iris-Versicolor vs Virginica\", \"BGD\", iris_versicolor_learn, iris_virginica_learn, iris_versicolor_test, iris_virginica_test, bgd_learn)\n",
    "calcualte_model(\"Iris-Versicolor vs Virginica\", \"SGD\", iris_versicolor_learn, iris_virginica_learn, iris_versicolor_test, iris_virginica_test, sgd_learn)\n",
    "calcualte_model(\"Iris-Versicolor vs Setonza\", \"BGD\", iris_versicolor_learn, iris_setosa_learn, iris_versicolor_test, iris_setosa_test, bgd_learn)\n",
    "calcualte_model(\"Iris-Versicolor vs Setonza\", \"SGD\", iris_versicolor_learn, iris_setosa_learn, iris_versicolor_test, iris_setosa_test, sgd_learn)\n",
    "calcualte_model(\"Iris-Virginica vs Iris-Versicolor\", \"BGD\", iris_virginica_learn, iris_versicolor_learn, iris_virginica_test, iris_versicolor_test, bgd_learn)\n",
    "calcualte_model(\"Iris-Virginica vs Iris-Versicolor\", \"SGD\", iris_virginica_learn, iris_versicolor_learn, iris_virginica_test, iris_versicolor_test, sgd_learn)\n",
    "calcualte_model(\"Iris-Virginica vs Iris-Setonza\", \"BGD\", iris_virginica_learn, iris_setosa_learn, iris_virginica_test, iris_setosa_test, bgd_learn)\n",
    "calcualte_model(\"Iris-Virginica vs Iris-Setonza\", \"SGD\", iris_virginica_learn, iris_setosa_learn, iris_virginica_test, iris_setosa_test, sgd_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
